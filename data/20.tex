iModern many-core processors combine deep memory hierarchies with a large number of cores, making both data locality and load balance critical for performance. Forkâ€“join constructs with static scheduling can preserve locality through fixed work-to-core mappings but may suffer severe load imbalance, whereas task-based runtimes balance irregular workloads via dynamic scheduling while offering limited control over task placement and execution order, often at the expense of locality. As different kernels may prioritize locality, load balance, or an intermediate trade-off, current scheduling mechanisms are insufficient to adapt to diverse and evolving application needs.

In this paper, we address this limitation by introducing a flexible scheduling framework built on two core abstractions: predefined scheduling policies and taskgroups. Taskgroups act as independent scheduling domains, allowing tasks from a specific kernel or subcomputation to be executed under a locally defined policy. By composing and nesting these domains, the framework enables complex and dynamic scheduling strategies, naturally supporting hierarchical and multi-kernel applications. The framework supports FIFO, LIFO, priority, and affinity policies and their combinations, with affinity exposed as a first-class mechanism to tune the trade-off between data locality and load balance.
We demonstrate the integration of the framework in the OmpSs-2 tasking model, while keeping the design general enough to be applicable to other programming models. An evaluation using synthetic benchmarks shows that application behavior and performance can vary substantially depending on the chosen scheduling policy, and that the proposed framework makes such choices easy to express and modify. Overall, this work provides a practical and extensible path toward application-tailored scheduling in task-based runtime systems. 
